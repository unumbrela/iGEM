{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import functools\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma  \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import amp.data_utils.data_loader as data_loader\n",
    "import amp.utils.classifier_utils as cu\n",
    "from amp.config import MAX_LENGTH, MIN_LENGTH, VOCAB_PAD_SIZE, LATENT_DIM\n",
    "from amp.data_utils import sequence as du_sequence\n",
    "from amp.utils import generator, basic_model_serializer\n",
    "from amp.utils.basic_model_serializer import load_master_model_components\n",
    "from amp.utils.seed import set_seed\n",
    "from amp.utils import phys_chem_propterties\n",
    "from amp.inference.filtering import amino_based_filtering         \n",
    "\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from keras import layers, activations, Model\n",
    "input_to_encoder = layers.Input(shape=(MAX_LENGTH,))\n",
    "input_to_decoder = layers.Input(shape=(LATENT_DIM+2,))\n",
    "\n",
    "import modlamp.descriptors\n",
    "import modlamp.analysis\n",
    "import modlamp.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_generated_peptide(encoded_peptide):\n",
    "    alphabet = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "    return ''.join([alphabet[el - 1] if el != 0 else \"\" for el in encoded_peptide.argmax(axis=1)])\n",
    "\n",
    "def translate_peptide(encoded_peptide):\n",
    "    alphabet = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "    return ''.join([alphabet[el-1] if el != 0 else \"\" for el in encoded_peptide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'HydrAMP',\n",
    "    'PepCVAE',\n",
    "    'Basic',\n",
    "]\n",
    "\n",
    "\n",
    "best_epochs = {\n",
    "    'HydrAMP': 37,\n",
    "    'PepCVAE': 35,\n",
    "    'Basic': 15,\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pszymczak/amp/new-tf/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/pszymczak/amp/new-tf/lib/python3.8/site-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "data_manager = data_loader.AMPDataManager(\n",
    "    '../data/unlabelled_positive.csv',\n",
    "    '../data/unlabelled_negative.csv',\n",
    "    min_len=MIN_LENGTH,\n",
    "    max_len=MAX_LENGTH)\n",
    "\n",
    "amp_x, amp_y = data_manager.get_merged_data()\n",
    "amp_x_train, amp_x_test, amp_y_train, amp_y_test = train_test_split(amp_x, amp_y, test_size=0.1, random_state=36)\n",
    "amp_x_train, amp_x_val, amp_y_train, amp_y_val = train_test_split(amp_x_train, amp_y_train, test_size=0.2, random_state=36)\n",
    "\n",
    "# Restrict the length\n",
    "ecoli_df = pd.read_csv('../data/mic_data.csv')\n",
    "mask = (ecoli_df['sequence'].str.len() <= MAX_LENGTH) & (ecoli_df['sequence'].str.len() >= MIN_LENGTH)\n",
    "ecoli_df = ecoli_df.loc[mask]\n",
    "mic_x = du_sequence.pad(du_sequence.to_one_hot(ecoli_df['sequence']))\n",
    "mic_y = ecoli_df.value\n",
    "\n",
    "\n",
    "mic_x_train, mic_x_test, mic_y_train, mic_y_test = train_test_split(mic_x, mic_y, test_size=0.1, random_state=36)\n",
    "mic_x_train, mic_x_val, mic_y_train, mic_y_val = train_test_split(mic_x_train, mic_y_train, test_size=0.2, random_state=36)\n",
    "\n",
    "new_train = pd.DataFrame()\n",
    "new_train['value'] = list(mic_y_train)\n",
    "new_train['sequence'] = list(mic_x_train)\n",
    "active_peptides = new_train[new_train.value < 1.5]\n",
    "active_peptides  = pd.concat([active_peptides] * 25)\n",
    "new_train  = pd.concat([\n",
    "    new_train,\n",
    "    active_peptides,    \n",
    "])\n",
    "mic_x_train = np.array(list(new_train.sequence)).reshape(len(new_train), 25)\n",
    "mic_y_train = np.array(new_train.value).reshape(len(new_train),)\n",
    "\n",
    "#180194\n",
    "uniprot_x_train = np.array(du_sequence.pad(du_sequence.to_one_hot(pd.read_csv('../data/Uniprot_0_25_train.csv').Sequence)))\n",
    "#22525\n",
    "uniprot_x_val = np.array(du_sequence.pad(du_sequence.to_one_hot(pd.read_csv('../data/Uniprot_0_25_val.csv').Sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bms = basic_model_serializer.BasicModelSerializer()\n",
    "amp_classifier = bms.load_model('../models/amp_classifier')\n",
    "amp_classifier_model = amp_classifier()\n",
    "mic_classifier = bms.load_model('../models/mic_classifier/')\n",
    "mic_classifier_model = mic_classifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16028/16028 [==============================] - 1s 62us/step\n",
      "16028/16028 [==============================] - 0s 23us/step\n",
      "4007/4007 [==============================] - 0s 2us/step\n",
      "4007/4007 [==============================] - 0s 12us/step\n",
      "40904/40904 [==============================] - 0s 2us/step\n",
      "40904/40904 [==============================] - 0s 5us/step\n",
      "620/620 [==============================] - 0s 9us/step\n",
      "620/620 [==============================] - 0s 85us/step\n",
      "180194/180194 [==============================] - 0s 2us/step\n",
      "180194/180194 [==============================] - 0s 1us/step\n",
      "22524/22524 [==============================] - 0s 2us/step\n",
      "22524/22524 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:01<00:00, 1312.01it/s]\n"
     ]
    }
   ],
   "source": [
    "#DATASET_AMP/MIC_TRAIN/VAL\n",
    "\n",
    "amp_amp_train = amp_classifier_model.predict(amp_x_train, verbose=1, batch_size=10000).reshape(len(amp_x_train))\n",
    "amp_mic_train = mic_classifier_model.predict(amp_x_train, verbose=1, batch_size=10000).reshape(len(amp_x_train))\n",
    "amp_amp_val = amp_classifier_model.predict(amp_x_val, verbose=1, batch_size=10000).reshape(len(amp_x_val))\n",
    "amp_mic_val = mic_classifier_model.predict(amp_x_val, verbose=1, batch_size=10000).reshape(len(amp_x_val))\n",
    "\n",
    "mic_amp_train = amp_classifier_model.predict(mic_x_train, verbose=1, batch_size=10000).reshape(len(mic_x_train))\n",
    "mic_mic_train = mic_classifier_model.predict(mic_x_train, verbose=1, batch_size=10000).reshape(len(mic_x_train))\n",
    "mic_amp_val = amp_classifier_model.predict(mic_x_val, verbose=1, batch_size=10000).reshape(len(mic_x_val))\n",
    "mic_mic_val = mic_classifier_model.predict(mic_x_val, verbose=1, batch_size=10000).reshape(len(mic_x_val))\n",
    "\n",
    "uniprot_x_train = np.array(du_sequence.pad(du_sequence.to_one_hot(pd.read_csv('../data/Uniprot_0_25_train.csv').Sequence)))\n",
    "uniprot_x_val = np.array(du_sequence.pad(du_sequence.to_one_hot(pd.read_csv('../data/Uniprot_0_25_val.csv').Sequence)))\n",
    "\n",
    "uniprot_amp_train = amp_classifier_model.predict(uniprot_x_train, verbose=1, batch_size=10000).reshape(len(uniprot_x_train))\n",
    "uniprot_mic_train = mic_classifier_model.predict(uniprot_x_train, verbose=1, batch_size=10000).reshape(len(uniprot_x_train))\n",
    "uniprot_amp_val = amp_classifier_model.predict(uniprot_x_val, verbose=1, batch_size=10000).reshape(len(uniprot_x_val))\n",
    "uniprot_mic_val = mic_classifier_model.predict(uniprot_x_val, verbose=1, batch_size=10000).reshape(len(uniprot_x_val))\n",
    "\n",
    "training_generator = generator.concatenated_generator(\n",
    "    uniprot_x_train,\n",
    "    uniprot_amp_train,\n",
    "    uniprot_mic_train,\n",
    "    amp_x_train,\n",
    "    amp_amp_train,\n",
    "    amp_mic_train,\n",
    "    mic_x_train,\n",
    "    mic_amp_train,\n",
    "    mic_mic_train,\n",
    "    128\n",
    ")\n",
    "\n",
    "x_train_complete = np.concatenate([next(training_generator)[0][0] for _ in tqdm(range(1408))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PCA and generate peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unconstrained(\n",
    "        n: int,\n",
    "        decoder,\n",
    "        amp_classifier,\n",
    "        mic_classifier,\n",
    "        latent_dim: int = 64,\n",
    "):\n",
    "    \n",
    "    z = np.random.normal(size=(n, latent_dim))\n",
    "    z = pca_decomposer.inverse_transform(z)\n",
    "    z = np.vstack([z]*latent_dim)\n",
    "    z_cond_pos = np.hstack([z, np.ones((n*latent_dim, 1)), np.ones((n*latent_dim, 1))])\n",
    "    pos_decoded = decoder.predict(z_cond_pos, batch_size=1000)\n",
    "    pos_decoded_argmax = pos_decoded.argmax(axis=2)\n",
    "    pos_peptides = [translate_generated_peptide(peptide) for peptide in pos_decoded]\n",
    "    pos_class_prediction = amp_classifier.predict(pos_decoded_argmax, batch_size=1000)\n",
    "    pos_mic_prediction = mic_classifier.predict(pos_decoded_argmax, batch_size=1000)\n",
    "    \n",
    "    z_cond_neg = np.hstack([z, np.zeros((n*latent_dim, 1)), np.zeros((n*latent_dim, 1))])\n",
    "    neg_decoded = decoder.predict(z_cond_neg, batch_size=1000)\n",
    "    neg_decoded_argmax = neg_decoded.argmax(axis=2)\n",
    "    neg_peptides = [translate_generated_peptide(peptide) for peptide in neg_decoded]\n",
    "    neg_class_prediction = amp_classifier.predict(neg_decoded_argmax, batch_size=1000)\n",
    "    neg_mic_prediction = mic_classifier.predict(neg_decoded_argmax, batch_size=1000)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'pos_peptides': pos_peptides,\n",
    "        'pos_class_prediction': pos_class_prediction,\n",
    "        'pos_mic_prediction': pos_mic_prediction,\n",
    "        'neg_peptides': neg_peptides,\n",
    "        'neg_class_prediction': neg_class_prediction,\n",
    "        'neg_mic_prediction': neg_mic_prediction,   \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'temperature:0' shape=() dtype=float32, numpy=0.1> temperature\n",
      "540672/540672 [==============================] - 277s 513us/step\n",
      "tracking <tf.Variable 'temperature:0' shape=() dtype=float32, numpy=0.1> temperature\n",
      "   128/540672 [..............................] - ETA: 11:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-07ce2d5a54c8>:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "  decoder_model = Model(input=decoder_model.input, output=[x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540672/540672 [==============================] - 268s 496us/step\n",
      "tracking <tf.Variable 'temperature:0' shape=() dtype=float32, numpy=0.6023884> temperature\n",
      "    32/540672 [..............................] - ETA: 30:26"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-07ce2d5a54c8>:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "  decoder_model = Model(input=decoder_model.input, output=[x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540672/540672 [==============================] - 263s 487us/step\n"
     ]
    }
   ],
   "source": [
    "n = 50000\n",
    "\n",
    "for model in models:\n",
    "    AMPMaster = bms.load_model(f'../models/{model}/{int(best_epochs[model])}')\n",
    "    encoder_model =  AMPMaster.encoder(input_to_encoder)\n",
    "    decoder_model = AMPMaster.decoder(input_to_decoder)\n",
    "    # Replace Gumbel with Softmax\n",
    "    if model in ['PepCVAE', 'Basic']:\n",
    "        new_act = layers.TimeDistributed(\n",
    "            layers.Activation(activations.softmax),\n",
    "            name='decoder_time_distribute_activation')\n",
    "        decoder_model.layers.pop()\n",
    "        x = new_act(decoder_model.layers[-1].output)\n",
    "        decoder_model = Model(input=decoder_model.input, output=[x]) \n",
    "    x_train_pred = encoder_model.predict(x_train_complete, verbose=1)\n",
    "    pca_decomposer = PCA()\n",
    "    pca_decomposer.fit_transform(x_train_pred)\n",
    "    dump(pca_decomposer, f'../models/{model}/pca_decomposer.joblib')\n",
    "    \n",
    "    model_results = generate_unconstrained(\n",
    "        n=n,\n",
    "        decoder=decoder_model,\n",
    "        amp_classifier=amp_classifier_model,\n",
    "        mic_classifier=mic_classifier_model,\n",
    "    )\n",
    "    dump(model_results, f'../results/unconstrained_{model}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_results = load(f'../results/unconstrained_{models[0]}.joblib')\n",
    "pepcvae_results = load(f'../results/unconstrained_{models[1]}.joblib')\n",
    "basic_results = load(f'../results/unconstrained_{models[2]}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_peptides(results):\n",
    "    peptides = np.array(results['pos_peptides']).reshape(64, -1).T\n",
    "    amp = (results['pos_class_prediction'] < 0.8).reshape(64, -1)\n",
    "    mic = results['pos_mic_prediction'].reshape(64, -1)\n",
    "    combined = ma.masked_where(amp, mic)\n",
    "    good = combined.argmax(axis=0)\n",
    "    good_peptides = peptides[list(range(peptides.shape[0])), good]\n",
    "    good_amp = np.array(results['pos_class_prediction']).reshape(64, -1).T[list(range(peptides.shape[0])), good]\n",
    "    good_mic = np.array(results['pos_mic_prediction']).reshape(64, -1).T[list(range(peptides.shape[0])), good]\n",
    "    return pd.DataFrame.from_dict({\n",
    "        'sequence': good_peptides.tolist(), \n",
    "        'amp': good_amp.tolist(),\n",
    "        'mic': good_mic.tolist(),\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_pd = select_peptides(hydra_results)\n",
    "pepcvae_pd = select_peptides(pepcvae_results)\n",
    "basic_pd = select_peptides(basic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_filtering(dataset):\n",
    "    dataset = dataset[(dataset['amp'] > 0.8) & (dataset['mic'] > 0.8)]\n",
    "    dataset = amino_based_filtering('../data/unlabelled_positive.csv', dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_clean = final_filtering(hydra_pd).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_chem = pd.DataFrame.from_dict(phys_chem_propterties.calculate_physchem_prop(hydra_clean.sequence.tolist()))\n",
    "phys_chem['sequence'] = hydra_clean.sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_phys = pd.merge(hydra_clean, phys_chem, on='sequence').drop(['index'], axis=1)\n",
    "hydra_phys = hydra_phys[hydra_phys['h_score'] > 1.5]\n",
    "hydra_final = hydra_phys.sort_values('h_score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_final.to_csv('../results/hydra_unconstrained.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
